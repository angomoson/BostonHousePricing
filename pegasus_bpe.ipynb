{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvdXkGIsVWUd5/MGBr8N6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angomoson/BostonHousePricing/blob/main/pegasus_bpe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMLErZzPCDXO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM, PreTrainedTokenizerFast, DataCollatorForSeq2Seq,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# ‚úÖ Ensure CUDA is available if using GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ‚úÖ Load the dataset\n",
        "dataset_path = \"your_dataset.csv\"  # Change this to your actual dataset path\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset file '{dataset_path}' not found!\")\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(dataset_path, encoding=\"utf-8\").dropna().reset_index(drop=True)\n",
        "\n",
        "# ‚úÖ Ensure the dataset has the required columns\n",
        "required_columns = {\"article\", \"summary\"}\n",
        "if not required_columns.issubset(df.columns):\n",
        "    raise ValueError(f\"Dataset must have columns: {required_columns}. Found: {df.columns}\")\n",
        "\n",
        "# ‚úÖ Convert dataset to Hugging Face format\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# ‚úÖ Step 1: Train a BPE Tokenizer\n",
        "print(\"Training BPE tokenizer...\")\n",
        "tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer_model.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "tokenizer_model.train(files=[\"your_dataset.txt\"], trainer=trainer)\n",
        "tokenizer_model.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "# ‚úÖ Step 2: Load Trained BPE Tokenizer\n",
        "print(\"Loading trained BPE tokenizer...\")\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"bpe_tokenizer.json\")\n",
        "tokenizer.pad_token = \"[PAD]\"\n",
        "tokenizer.unk_token = \"[UNK]\"\n",
        "tokenizer.cls_token = \"[CLS]\"\n",
        "tokenizer.sep_token = \"[SEP]\"\n",
        "tokenizer.mask_token = \"[MASK]\"\n",
        "\n",
        "# ‚úÖ Step 3: Tokenize Dataset\n",
        "def tokenize_function(examples, max_length=128):\n",
        "    \"\"\"Tokenizes input articles and summaries for mBART\"\"\"\n",
        "    inputs = tokenizer(examples[\"article\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    targets = tokenizer(examples[\"summary\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# ‚úÖ Step 4: Split dataset into training & validation sets\n",
        "split_ratio = 0.9\n",
        "train_size = int(len(tokenized_dataset) * split_ratio)\n",
        "train_dataset = tokenized_dataset.select(range(train_size))\n",
        "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
        "\n",
        "print(f\"Dataset split: {len(train_dataset)} training samples, {len(eval_dataset)} validation samples.\")\n",
        "\n",
        "# ‚úÖ Step 5: Load the mBART Model for Summarization\n",
        "model_name = \"facebook/mbart-large-50\"\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# ‚úÖ Step 6: Use Data Collator for Padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# ‚úÖ Step 7: Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mbart_summarization\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 8: Use `evaluate` Instead of `load_metric`\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes ROUGE scores for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return {k: round(v.mid.fmeasure * 100, 2) for k, v in result.items()}\n",
        "\n",
        "# ‚úÖ Step 9: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 10: Start Training\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# ‚úÖ Step 11: Save the Fine-Tuned Model\n",
        "print(\"\\n‚úÖ Training complete! Saving model...\")\n",
        "model.save_pretrained(\"mbart_finetuned\")\n",
        "tokenizer.save_pretrained(\"mbart_finetuned\")\n",
        "\n",
        "# ‚úÖ Step 12: Test the Fine-Tuned Model\n",
        "print(\"\\nüîπ Testing model with a sample article...\")\n",
        "sample_text = \"Your sample article text here\"  # Replace with actual text\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
        "\n",
        "# Generate summary\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=50)\n",
        "\n",
        "summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nüìù Generated Summary:\", summary)\n"
      ],
      "metadata": {
        "id": "6qggwDzQCEsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}